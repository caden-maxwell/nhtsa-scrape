[Back to Documentation Overview](README.md)

---

# What I've Been Working On

| Date | Hours | What I Did |
| :---: | :---: | --- |
| 5/24 | 3.5 | - Sifted through the files of the old webscraper tool to better understand what I am doing. <br> - Looked through lots of information regarding the NASS/CDS database to figure out a better way to collect the data. <br> - Did some research on what the best tool would be for this project. Settled on the PyQt6 framework for Python. <br> - Created this GitHub repo. | 
| 5/25 | 1.5 | - Researching the best way to put this program into the hands of those who don't know how to code. Figure that it might just be easiest to make users install Python and run it themselves (they will also need to install all of the libraries). Will continue to do research on this front... |
| 5/26 | 6 | - More research on how the NHTSA formats and organizes their data in the NASS/CDS datasets. For now, I think it's easiest to gather the data the way it's done in the old app. Later on, I might look at utilizing the NHTSA's FTP site to download the NASS/CDS data directly, but parsing all of these years of data into something useful will be a nightmare since some years had different organization schemes than others. Notably, year ranges 1979-1987, 1988-1996, 1997-2008, and 2009-2015 are quite different from each other in terms of how the data is organized. However, a positive to doing it the FTP way is that it only has to be downloaded and parsed once, since the NASS/CDS database is no longer updated (replaced in 2016 by CISS, which *also* uses a different organization scheme). This would enable the user to query the data in a fraction of the time it takes to do so when it's contained in an online database. <br> - Started thinking about how the GUI will look and put together a small test run to figure out how PyQt6 functions. <br> - Started on a software development plan, completed the requirements gathering phase (for now). |
| 5/31 | 4 | - Dug into the HTML and Javascript for the NASS/CDS Crash Viewer to see what can be done to gather the data more easily. I think I figured out a better way to query the database without having to use Selenium! <br> - Re-formatted/organized the software development plan documents and created some new ones that may be needed later on. <br> - Updated all documentation to make navigation easier. |
| 6/1 | 3 | - Browsed over the old application to catch more of the requirements for the project. I assume that what functionality is there should still be present in the new app. <br> - Updated the requirements specification document to better reflect what is needed. Functional and non-functional requirements are now separate and much more detailed. <br> - Created example documents from the old project (scatterplot and CSV) that should be used as a guide for the new project. <br> - Started on system design |
| 6/9 | 4 | - Completed system design and corresponding data flow chart. I now have a *much* better understanding of how everything needs to be laid out in the final product, so things will be much easier from here. <br> - Spent some time creating a small program to see if it's actually worth it to parallelize requests as opposed to just doing them sequentially (it is **definitely** worth it... 5-10x speedups). Demo code is [`here`](../demos/parallel_req.py). <br> - Sent an email to the NHTSA support desk inquiring on rate limits for the CrashViewer database. <br> - Started on user interface design. I figure that this part doesn't matter too much since we just need a fast, working product. |
| 6/10 | 3 | - Laid all the groundwork for user interface design. There are still some elements that need to be a little more fleshed out (for example, I'm still not exactly sure how the data viewer should look), but I think I am 95% of the way there. <br> - Moving on to database designing. This shouldn't take too long. |
| 6/13 | 4.5 | - Got a relatively vague email back from the NHTSA regarding rate limits, so I sent another, more detailed inquiry. <br> - Did some research on SQLite and databasing in general. Created a small demo program to see if SQLite3 will actually be a good option for the project. Demo code is [`here`](../demos/sqlite_db.py). It's actually pretty simple to use, so I think I'll go with it. <br> - Coming up with a good database design was harder than I thought, but it's finished and now I can finally start on implementation! |
| 6/14 | 8.5 | - Created the general layout for the main window, logs window, and saving window in the PyQt6 designer. This program definitely has a bit of a steep learning curve but I'm getting better at it. <br> - Set up all slots/signals so that each page can be accessed via buttons. <br> - Refactored `MainWindow.ui` to separate `.ui` files and restructured the project files to be more consistent. |
| 6/15 | 7 | - UI is 90% done, just have to make sure I have all the necessary options for the scrape menu. <br> - Not sure what I need to put in the settings page or if I actually need it at all, but I'll leave it in there for now. <br> - I just realized I need to implement some parallelization for the loading screen, will do that first thing tomorrow. <br> - Need to implement a way to get data to the scrape menu from the profile menu. |
| 6/19 | 6 | - Added database table for the case profiles <br> - Added ability to view case profiles from the ListView in "Open existing profile" submenu. <br> - Added ability to delete profiles, rescrape a profile, and open the profile, and the ability to pass any amount of data between two pages as needed. Lots of key components are now in place. |
| 6/20 | 8 | - Added necessary elements for request handling <br> - Added auto-scrape from website dropdown fields upon startup, along with automatic field population and updating the vehicle model when the vehicle make is changed. <br> - Started on main scraping engine. There should be some copy/paste from the old application, however a lot still needs to be refactored to fit into the new app. |
| 6/21 | 2 | - Added dropdown 'values' to application. We need these, since the POST request to query the database uses these, not the literal make/model/etc names. |
| 6/22 | 7 | - Querying the database can now be done. <br> - Added a bracketed rate limiting system. The larger the amount of requests, the more the requests are spaced out to reduce load on server. <br> - Added ability for user to limit the number of cases retrieved. <br> - Added ability for user to change the default image set to be requested. <br> - Scraper now searches through each requested page of cases and gets individual case IDs. <br> - Individual cases can now be requested but have yet to be parsed. I imagine that I will be able to use a lot of the old app's code for the rest of the parsing and for the creation of the csv files and scatterplots. |
| 6/27 | 10 | - Added logging colors to make things stand out more. <br> - Figuring out how to parse the cases based on `old_app.py`... LOTS of manually sifting through XML to figure out how it is structured and TONS of refactoring to make the code more readable. |
| 6/28 | 8 | - The main scraping process is finished! In fact, when comparing output from the old scrape and the new one and double-checking with the raw XML, the new scraper collects information from *all* events, whereas the old one misses a couple here and there because of the way the code was written. <br> - Now need to be able to add the data to the data viewer and run some calculations on it. |
| 6/29 | 6 | - Moved to a different request handler that works 10x better and is more flexible than the last. <br> - Modularized the main scrape engine to be more readable and easy to debug. |
| 6/30 | 6 | - Extra cases that are above the max case limit will be put on a list to be enqueued if the current cases fail to obtain sufficient data. This way we can get as many cases as possible under the max case limit. <br> - Added model class to handle case events in the data viewer. <br> - Scraped data is now saved to the database automatically upon scraping. Need to implement it so that a new profile is automatically created at scrape time. <br> - Very happy with how this is turning out. |
| 7/2 | 7 | - Connected all the database tables. A new profile is created at scrape time, and events are added to it as they are scraped and processed. <br> - The case_events table is completely setup, and data viewer now populates with this data. <br> - Opening an existing scrape profile will now actually show the data that was scraped. <br> - Slightly changed database schema to make it easier to query certain things. <br> - Trying very hard to figure out what is going on with the plotting/csv part of the code. Pretty messy.|
| 7/4 | 4 | - Added ability to view and export the scatterplot and csv data. <br> - After adding the scatterplot, I was seeing some pretty weird data points, so I went back to the main scraper and fixed some bugs and added some more edge case handling. |
| 7/10 | 7 | - Added interactive scatterplot viewer. <br> - Scatterplot data and case labels can now be toggled. <br> - Scatterplot is saved to a unique scrape folder; separate saves do not overwrite each other. <br> - Multiple scrape profiles can be selected and opened/deleted at the same time. |
| 7/12 | 6 | - Added event view for each event/case. <br> - Event view now dynamically loads case images, however it is still slow when scraping is ongoing. I am going to implement a way to pause scraping and resume when the user wants to. <br> - Added ability to view the case images in the event viewer. <br> - There isn't that much left to do besides some functionality to save the images. Another thing I probably need to do is be able to get images from the user's machine, if present, and use those in the event viewer instead of making redundant image requests. |
| 7/13 | 1 | - Made it so that the request handler clears its own queue of images once new vehicle images are requested. Will probably change this later. <br> Fixed up some redundant code. |
| 7/16 | 5 | - Made event viewer UI less cluttered, only included absolutely necessary information for this page; other info will be made available in the table view tab. <br> - Moved functionality to get the image set from the scrape page to the event viewer page. <br> - Changed scrape menu layout to accomodate the above change. |
| 7/17 | 4 | - Events are now properly deleted with foreign key constraints in mind. <br> - Refactored some methods in the events tab file because everything was too confusing. <br> - Added some checks to make sure images weren't downloaded twice and that each case page was being cached correctly. |
| 7/21 | 7 | - Added ability to start/stop scraping images per case event. <br> - Fixed foreign key constraint upon deletion of event. <br> - Buttons on image thumbnails no longer disappear when adding new images to an event. <br> - Went into the office for some feedback on the current state of the project: <br> - - Needed to make DeltaV be mph instead of kmph by default, and to make this known on the scrape menu. <br> - - Need to figure out what is causing my app to discard some valid collision events. <br> - - Need to add a way to "ignore" events for now and add them back later if necessary. <br> - - Need to add the second NHTSA database (CISS). <br> - - Need to figure out if average crush calculation is off (splicing using 4 instead of 5 in the calculation). <br> - - Make the scatterplot update dynamically. <br> - - The standalone executable is only needed for Windows, no longer necessary to provide for MacOS. |
| 7/22 | 1 | - Figured out what was causing valid collision events to be discarded and fixed the bug. <br> - Fixed the scatterplot update issue. |
| 8/1 | 2 | - Made it so that any event can be ignored. <br> - The scatterplot now disregards ignored events. |
| 8/11 | 2 | - Fixed an error caused by a directory not being found to save data to in the standalone executable on Windows. <br> - Fixed an edge case that caused a crash when the CSV was already open in another application at the same time as writing. |
| 8/12 | 4 | - UI changes in the scrape menu to make things less cluttered. <br> - Created a new model for the table viewer and tackled bugs related to that. <br> - Table for data is now viewable and can be saved as a CSV in the same way that the scatterplot is saved. <br> - Still need to add some additional information to the CSV like the bottom summary, image ID, etc. |
| 8/14 | 6 | - Fixed a bug that would reset the "ignored" tag on previously scraped data when scraping a new profile that had some of the same events. <br> - Set up a new database handler to get ready for a second scrape engine (for the CISS database). |
| 8/19 | 8 | - Added rate limit settings to settings page. <br> - Added <code>settings.json</code>. <br> - Integrated rate limit settings to automatically save to file and update the request handler. <br> - Made an absolute minimum limit to the rate limiter just in case someone is careless (0.2s). <br> - TODO for tomorrow: Add scrape menu params and statistics to scrape summary tab in data viewer. <br> Another TODO for tomorrow: Add something to keep track of which case events failed to parse and the reason it failed (maybe add another tab for this called "Failed Parses" or something). |
| 8/20 | N/A | N/A |
| Total | 158 | --- |

---

[Back to Documentation Overview](README.md)