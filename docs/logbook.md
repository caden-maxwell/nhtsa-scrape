[Back to Documentation Overview](README.md)

---

# What I've Been Working On

| Date | Hours | What I Did |
| :---: | :---: | --- |
| 5/24 | 3.5 | - Sifted through the files of the old webscraper tool to better understand what I am doing. <br> - Looked through lots of information regarding the NASS/CDS database to figure out a better way to collect the data. <br> - Did some research on what the best tool would be for this project. Settled on the PyQt6 framework for Python. <br> - Created this GitHub repo. | 
| 5/25 | 1.5 | - Researching the best way to put this program into the hands of those who don't know how to code. Figure that it might just be easiest to make users install Python and run it themselves (they will also need to install all of the libraries). Will continue to do research on this front... |
| 5/26 | 6 | - More research on how the NHTSA formats and organizes their data in the NASS/CDS datasets. For now, I think it's easiest to gather the data the way it's done in the old app. Later on, I might look at utilizing the NHTSA's FTP site to download the NASS/CDS data directly, but parsing all of these years of data into something useful will be a nightmare since some years had different organization schemes than others. Notably, year ranges 1979-1987, 1988-1996, 1997-2008, and 2009-2015 are quite different from each other in terms of how the data is organized. However, a positive to doing it the FTP way is that it only has to be downloaded and parsed once, since the NASS/CDS database is no longer updated (replaced in 2016 by CISS, which *also* uses a different organization scheme). This would enable the user to query the data in a fraction of the time it takes to do so when it's contained in an online database. <br> - Started thinking about how the GUI will look and put together a small test run to figure out how PyQt6 functions. <br> - Started on a software development plan, completed the requirements gathering phase (for now). |
| 5/31 | 4 | - Dug into the HTML and Javascript for the NASS/CDS Crash Viewer to see what can be done to gather the data more easily. I think I figured out a better way to query the database without having to use Selenium! <br> - Re-formatted/organized the software development plan documents and created some new ones that may be needed later on. <br> - Updated all documentation to make navigation easier. |
| 6/1 | 3 | - Browsed over the old application to catch more of the requirements for the project. I assume that what functionality is there should still be present in the new app. <br> - Updated the requirements specification document to better reflect what is needed. Functional and non-functional requirements are now separate and much more detailed. <br> - Created example documents from the old project (scatterplot and CSV) that should be used as a guide for the new project. <br> - Started on system design |
| 6/9 | 4 | - Completed system design and corresponding data flow chart. I now have a *much* better understanding of how everything needs to be laid out in the final product, so things will be much easier from here. <br> - Spent some time creating a small program to see if it's actually worth it to parallelize requests as opposed to just doing them sequentially (it is **definitely** worth it... 5-10x speedups). Demo code is [`here`](../demos/parallel_req.py). <br> - Sent an email to the NHTSA support desk inquiring on rate limits for the CrashViewer database. <br> - Started on user interface design. I figure that this part doesn't matter too much since we just need a fast, working product. |
| 6/10 | 3 | - Laid all the groundwork for user interface design. There are still some elements that need to be a little more fleshed out (for example, I'm still not exactly sure how the data viewer should look), but I think I am 95% of the way there. <br> - Moving on to database designing. This shouldn't take too long. |
| 6/13 | 4.5 | - Got a relatively vague email back from the NHTSA regarding rate limits, so I sent another, more detailed inquiry. <br> - Did some research on SQLite and databasing in general. Created a small demo program to see if SQLite3 will actually be a good option for the project. Demo code is [`here`](../demos/sqlite_db.py). It's actually pretty simple to use, so I think I'll go with it. <br> - Coming up with a good database design was harder than I thought, but it's finished and now I can finally start on implementation! |
| 6/14 | 8.5 | - Created the general layout for the main window, logs window, and saving window in the PyQt6 designer. This program definitely has a bit of a steep learning curve but I'm getting better at it. <br> - Set up all slots/signals so that each page can be accessed via buttons. <br> - Refactored `MainWindow.ui` to separate `.ui` files and restructured the project files to be more consistent. |
| 6/15 | 7 | - UI is 90% done, just have to make sure I have all the necessary options for the scrape menu. <br> - Not sure what I need to put in the settings page or if I actually need it at all, but I'll leave it in there for now. <br> - I just realized I need to implement some parallelization for the loading screen, will do that first thing tomorrow. <br> - Need to implement a way to get data to the scrape menu from the profile menu. |
| 6/19 | 6 | - Added database table for the case profiles <br> - Added ability to view case profiles from the ListView in "Open existing profile" submenu. <br> - Added ability to delete profiles, rescrape a profile, and open the profile, and the ability to pass any amount of data between two pages as needed. Lots of key components are now in place. |
| 6/20 | 8 | - Added necessary elements for request handling <br> - Added auto-scrape from website dropdown fields upon startup, along with automatic field population and updating the vehicle model when the vehicle make is changed. <br> - Started on main scraping engine. There should be some copy/paste from the old application, however a lot still needs to be refactored to fit into the new app. |
| 6/21 | 2 | - Added dropdown 'values' to application. We need these, since the POST request to query the database uses these, not the literal make/model/etc names. |
| 6/22 | 7 | - Querying the database can now be done. <br> - Added a bracketed rate limiting system. The larger the amount of requests, the more the requests are spaced out to reduce load on server. <br> - Added ability for user to limit the number of cases retrieved. <br> - Added ability for user to change the default image set to be requested. <br> - Scraper now searches through each requested page of cases and gets individual case IDs. <br> - Individual cases can now be requested but have yet to be parsed. I imagine that I will be able to use a lot of the old app's code for the rest of the parsing and for the creation of the csv files and scatterplots. |
| 6/27 | 10 | - Added logging colors to make things stand out more. <br> - Figuring out how to parse the cases based on `old_app.py`... LOTS of manually sifting through XML to figure out how it is structured and TONS of refactoring to make the code more readable. |
| 6/28 | 8 | - The main scraping process is finished! In fact, when comparing output from the old scrape and the new one and double-checking with the raw XML, the new scraper collects information from *all* events, whereas the old one misses a couple here and there because of the way the code was written. <br> - Now need to be able to add the data to the data viewer and run some calculations on it. |
| 6/29 | 6 | - Moved to a different request handler that works 10x better and is more flexible than the last. <br> - Modularized the main scrape engine to be more readable and easy to debug. |
| 6/30 | 6 | - Extra cases that are above the max case limit will be put on a list to be enqueued if the current cases fail to obtain sufficient data. This way we can get as many cases as possible under the max case limit. <br> - Added model class to handle case events in the data viewer. <br> - Scraped data is now saved to the database automatically upon scraping. Need to implement it so that a new profile is automatically created at scrape time. <br> - Very happy with how this is turning out. |
| 7/2 | 7 | - Connected all the database tables. A new profile is created at scrape time, and events are added to it as they are scraped and processed. <br> - The case_events table is completely setup, and data viewer now populates with this data. <br> - Opening an existing scrape profile will now actually show the data that was scraped. <br> - Slightly changed database schema to make it easier to query certain things. <br> - Trying very hard to figure out what is going on with the plotting/csv part of the code. Pretty messy.|
| 7/4 | 4 | - Added ability to view and export the scatterplot and csv data. <br> - After adding the scatterplot, I was seeing some pretty weird data points, so I went back to the main scraper and fixed some bugs and added some more edge case handling. |
| 7/10 | 7 | - Added interactive scatterplot viewer. <br> - Scatterplot data and case labels can now be toggled. <br> - Scatterplot is saved to a unique scrape folder; separate saves do not overwrite each other. <br> - Multiple scrape profiles can be selected and opened/deleted at the same time. |
| Total | 118 | - |

---

[Back to Documentation Overview](README.md)